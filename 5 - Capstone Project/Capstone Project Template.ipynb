{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Citi Bikes in New York City\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project designs an analytical dataware house and corresponding ETLs for the **bicycle sharing system *Citi Bikes*** in New York City. The project combines **bike trip data** provided *Citi Bikes* with **weather data of New York City**. The data warehouse is designed to enable data analysts and data scientist of *Citi Bikes* to perform different analysis on usage data especially in relation to weather impact.\n",
    "\n",
    "---\n",
    "\n",
    "This project follows the following steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "The scope of this project is to design a hypothetical single-source-of-truth database for usage data of the **bicycle sharing system *Citi Bikes*** in New York City. Using Python and Spark **trip data provided by *City Bikes* on S3** and ***weather data provided by the *Open-Meteo* API*** is merged into a relational database. Potential data scientists using the database should be able to answer various different questions: Does bad weather influence rental numbers? What are the popular routes taken on a given day of the week by customers? \n",
    "\n",
    "#### Data \n",
    "\n",
    "The data model consists of bike trip data and weather data.\n",
    "\n",
    "#### 1.) Trip data\n",
    "\n",
    "*Citi Bikes* provides its bike trip as open source data (https://ride.citibikenyc.com/system-data). For this project the 2016 data is used. The data is stored in monthly batches on a public S3 bucket. Regarding to content the data contains all bike trips taken by *Citi Bikes* customers in 2016 in New York City. Informations on these trips include - among other things - demographical customer data, start point and end point of the trip. Each monthly batch contains more than around 500,000 bike trips.\n",
    "\n",
    "#### 2.) Weather data\n",
    "\n",
    "As weather usually has a big impact on bike usage, daily 2016 weather data for New York City is included the database. The data is queried from the open-source weather API *Open-Meteo* (https://open-meteo.com/). It includes daily information on like maximum temperature or hours of precipitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Setup\n",
    "\n",
    "Importing all needed packages and defining reoccuring generic functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import of need packages\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os  \n",
    "import glob\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "# Generic function returning current time for logging\n",
    "def current_time():\n",
    "    \"\"\"\n",
    "    Return current time in hh:mm:ss\n",
    "    \"\"\"\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    return(current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Bike trip data\n",
    "\n",
    "The 2016 *Citi Bikes* trip data is queried from S3 and safed as a .csv in /stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:35:28 - Starting to parse 3 S3 urls\n",
      "16:35:28 - Reading https://s3.amazonaws.com/tripdata/201601-citibike-tripdata.zip\n",
      "16:35:31 - Writing stage/trip_data/201601-citibike-tripdata.csv\n",
      "16:35:38 - Reading https://s3.amazonaws.com/tripdata/201602-citibike-tripdata.zip\n",
      "16:35:42 - Writing stage/trip_data/201602-citibike-tripdata.csv\n",
      "16:35:50 - Reading https://s3.amazonaws.com/tripdata/201603-citibike-tripdata.zip\n",
      "16:35:55 - Writing stage/trip_data/201603-citibike-tripdata.csv\n",
      "16:36:08 - Finished to parse 3 S3 urls\n"
     ]
    }
   ],
   "source": [
    "# list of all 2016 New York City citibike tripdata files on S3. \n",
    "# In order to keep the script running smoothly only data from Q1 is loaded.\n",
    "\n",
    "trip_data_urls = ['https://s3.amazonaws.com/tripdata/201601-citibike-tripdata.zip',\n",
    "                  'https://s3.amazonaws.com/tripdata/201602-citibike-tripdata.zip',\n",
    "                  'https://s3.amazonaws.com/tripdata/201603-citibike-tripdata.zip',\n",
    "                 ]\n",
    "\n",
    "print(current_time(), '- Starting to parse', len(trip_data_urls), 'S3 urls')\n",
    "\n",
    "# Create a staging-folder if it not already exists\n",
    "os.makedirs('stage/trip_data', exist_ok=True)  \n",
    "\n",
    "# Looping all S3 files: Reading, unzip and save as a .csv in /stage\n",
    "for elem in trip_data_urls:\n",
    "    \n",
    "    # read file from S3\n",
    "    print(current_time(), \"- Reading\", elem)\n",
    "    df = pd.read_csv(elem, compression='zip', header=0, sep=',', quotechar='\"')\n",
    "    \n",
    "    # write file to .csv in stage\n",
    "    write_to_path = 'stage/trip_data/' + elem.split('/')[-1].split('.')[0] + '.csv'\n",
    "    print(current_time(), \"- Writing\", write_to_path)\n",
    "    df.to_csv(write_to_path)\n",
    "    \n",
    "print(current_time(), '- Finished to parse', len(trip_data_urls), 'S3 urls')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Lets inspect the df of the last interation to get an idea what the raw trip data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tripduration          starttime           stoptime  start station id  \\\n",
      "0          1491  3/1/2016 06:52:42  3/1/2016 07:17:33                72   \n",
      "1          1044  3/1/2016 07:05:50  3/1/2016 07:23:15                72   \n",
      "2           714  3/1/2016 07:15:05  3/1/2016 07:26:59                72   \n",
      "3           329  3/1/2016 07:26:04  3/1/2016 07:31:34                72   \n",
      "4          1871  3/1/2016 07:31:30  3/1/2016 08:02:41                72   \n",
      "\n",
      "  start station name  start station latitude  start station longitude  \\\n",
      "0   W 52 St & 11 Ave               40.767272               -73.993929   \n",
      "1   W 52 St & 11 Ave               40.767272               -73.993929   \n",
      "2   W 52 St & 11 Ave               40.767272               -73.993929   \n",
      "3   W 52 St & 11 Ave               40.767272               -73.993929   \n",
      "4   W 52 St & 11 Ave               40.767272               -73.993929   \n",
      "\n",
      "   end station id          end station name  end station latitude  \\\n",
      "0             427       Bus Slip & State St             40.701907   \n",
      "1             254           W 11 St & 6 Ave             40.735324   \n",
      "2             493           W 45 St & 6 Ave             40.756800   \n",
      "3             478          11 Ave & W 41 St             40.760301   \n",
      "4             151  Cleveland Pl & Spring St             40.722104   \n",
      "\n",
      "   end station longitude  bikeid    usertype  birth year  gender  \n",
      "0             -74.013942   23914  Subscriber      1982.0       1  \n",
      "1             -73.998004   23697  Subscriber      1978.0       1  \n",
      "2             -73.982912   21447  Subscriber      1960.0       2  \n",
      "3             -73.998842   22351  Subscriber      1986.0       1  \n",
      "4             -73.997249   20985  Subscriber      1978.0       1  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Weather data\n",
    "\n",
    "The daily 2016 weather for NYC is queried from the *Open-Meteo API* and saved as a .csv in /stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:36:08 - Reading weather API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "# API url of daily 2016 weather data of New York city\n",
    "\n",
    "try:\n",
    "    print(current_time(), \"- Reading weather API\")\n",
    "    url_api = \"https://archive-api.open-meteo.com/v1/era5?latitude=40.71&longitude=-74.01&start_date=2016-01-01&end_date=2016-12-31&daily=temperature_2m_max,precipitation_sum,precipitation_hours&timezone=America%2FNew_York\"\n",
    "    \n",
    "    # request data from api in .json format \n",
    "    json_weather_nyc = requests.get(url_api, verify=False).json()\n",
    "    df_weather_nyc = pd.DataFrame.from_records(json_weather_nyc['daily'])\n",
    "\n",
    "except:\n",
    "    raise Exception(\"Downloading data from API failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:36:09 - Writing stage/weather_data/2016-nyc-weather.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a staging-folder if it not already exists\n",
    "os.makedirs('stage/weather_data', exist_ok=True)  \n",
    "\n",
    "# Writing data to stage\n",
    "write_to_path = 'stage/weather_data/2016-nyc-weather.csv'\n",
    "print(current_time(), \"- Writing\", write_to_path)\n",
    "df_weather_nyc.to_csv(write_to_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Inspect the df to get an idea what the raw weather data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   precipitation_hours  precipitation_sum  temperature_2m_max        time\n",
      "0                  0.0                0.0                 6.2  2016-01-01\n",
      "1                  0.0                0.0                 4.2  2016-01-02\n",
      "2                  0.0                0.0                 6.5  2016-01-03\n",
      "3                  0.0                0.0                -0.4  2016-01-04\n",
      "4                  0.0                0.0                -1.4  2016-01-05\n"
     ]
    }
   ],
   "source": [
    "print(df_weather_nyc.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "\n",
    "In step 2 we explore our two data sets, look for possible data quality issues are examined and clean the data if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:36:19 - Spark session created or already running\n"
     ]
    }
   ],
   "source": [
    "# start spark session\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "    config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "    enableHiveSupport().getOrCreate()\n",
    "    print(current_time(), \"- Spark session created or already running\")\n",
    "except:\n",
    "    raise Exception(\"Spark session could not been created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Trip data\n",
    "\n",
    "As the trip data sets can be quite large Spark is utilized to explore and clean the data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:36:19 - Started reading staging trip data into spark dataframe\n",
      "16:36:26 - Finished reading 1990273 rows from trip staging data into spark dataframe\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- tripduration: string (nullable = true)\n",
      " |-- starttime: string (nullable = true)\n",
      " |-- stoptime: string (nullable = true)\n",
      " |-- start station id: string (nullable = true)\n",
      " |-- start station name: string (nullable = true)\n",
      " |-- start station latitude: string (nullable = true)\n",
      " |-- start station longitude: string (nullable = true)\n",
      " |-- end station id: string (nullable = true)\n",
      " |-- end station name: string (nullable = true)\n",
      " |-- end station latitude: string (nullable = true)\n",
      " |-- end station longitude: string (nullable = true)\n",
      " |-- bikeid: string (nullable = true)\n",
      " |-- usertype: string (nullable = true)\n",
      " |-- birth year: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read and combine all .csv-files of the trip data into one spark df\n",
    "print(current_time(), \"- Started reading staging trip data into spark dataframe\")\n",
    "\n",
    "df_trip = spark.read \\\n",
    "    .option(\"header\",True) \\\n",
    "    .csv('stage/trip_data/')\n",
    "\n",
    "print(current_time(), \"- Finished reading\", df_trip.count(), \"rows from trip staging data into spark dataframe\")\n",
    "\n",
    "# Inspect the data structure of the trip data\n",
    "df_trip.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+-----------------+------------------+-----------------+\n",
      "|summary|     tripduration|        starttime|         stoptime|  start station id|   end station id|\n",
      "+-------+-----------------+-----------------+-----------------+------------------+-----------------+\n",
      "|  count|          1990273|          1990273|          1990273|           1990273|          1990273|\n",
      "|   mean|935.4001752523397|             null|             null| 843.9573872529045|830.3775070053204|\n",
      "| stddev|8735.268620734047|             null|             null|1004.2269454016665|992.2558578543673|\n",
      "|    min|              100|1/1/2016 00:00:41|1/1/2016 00:06:51|               116|              116|\n",
      "|    max|             9998|3/9/2016 23:59:56|4/9/2016 19:22:49|                83|               83|\n",
      "+-------+-----------------+-----------------+-----------------+------------------+-----------------+\n",
      "\n",
      "+-------+------------------+----------------------+-----------------------+----------------+--------------------+-----------------------+\n",
      "|summary|start station name|start station latitude|start station longitude|end station name|end station latitude|start station longitude|\n",
      "+-------+------------------+----------------------+-----------------------+----------------+--------------------+-----------------------+\n",
      "|  count|           1990273|               1990273|                1990273|         1990273|             1990273|                1990273|\n",
      "|   mean|              null|    40.737596191232754|     -73.98704125225659|            null|   40.73682997001898|     -73.98704125225659|\n",
      "| stddev|              null|  0.021682949555749766|   0.015371753721201075|            null| 0.14881241953112853|   0.015371753721201075|\n",
      "|    min|   1 Ave & E 15 St|           40.67890679|     -73.92989109999999| 1 Ave & E 15 St|                 0.0|     -73.92989109999999|\n",
      "|    max|  York St & Jay St|           40.78720869|           -74.01713445|York St & Jay St|         40.78720869|           -74.01713445|\n",
      "+-------+------------------+----------------------+-----------------------+----------------+--------------------+-----------------------+\n",
      "\n",
      "+-------+------------------+----------+------------------+------------------+\n",
      "|summary|            bikeid|  usertype|        birth year|            gender|\n",
      "+-------+------------------+----------+------------------+------------------+\n",
      "|  count|           1990273|   1990273|           1842659|           1990273|\n",
      "|   mean|20080.173777165244|      null|1976.4743775164043|1.1284577542879797|\n",
      "| stddev|3010.7525125608995|      null| 11.77944813919537|0.5144200720611252|\n",
      "|    min|             14529|  Customer|            1885.0|                 0|\n",
      "|    max|             24773|Subscriber|            2000.0|                 2|\n",
      "+-------+------------------+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect summary of \"real\" trip data in the trip data\n",
    "df_trip.describe(['tripduration','starttime', 'stoptime','start station id', 'end station id']).show()\n",
    "\n",
    "# Inspect summary of station data\n",
    "df_trip.describe(['start station name','start station latitude', 'start station longitude','end station name','end station latitude', 'start station longitude']).show()\n",
    "\n",
    "# Inspect summary of customer data within the trip data\n",
    "df_trip.describe(['bikeid','usertype', 'birth year','gender']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Inspection of the data shows that **trip duration is in a reasonable range** (100 seconds to ca. 2.5 hours). Also **missing data** seems to be no problem according to the counts. Only for *birth year* data is missing. However this seems to be no data quality issue as *Citi Bikes* also serves anonymous customers.\n",
    "\n",
    "The only problem we can identify is **unreasonably low *birth year*** (e.g. 1885). By now the customer would over 130 years old. Therefore all *birth years* below 1916 are set to missing as these data points are most likely wrong and would skew later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:37:59 Count birth year below 1916 before cleaning: 766\n",
      "16:38:05 Count birth year missing before cleaning: 147614\n",
      "16:38:12 Count birth year below 1916 after cleaning: 0\n",
      "16:38:17 Count birth year missing after cleaning: 148380\n"
     ]
    }
   ],
   "source": [
    "# How many people are over 100 years old?\n",
    "print(current_time(),'Count birth year below 1916 before cleaning:',\n",
    "      df_trip.select('birth year').where(df_trip['birth year'] < 1916).count())\n",
    "\n",
    "# How many birth dates are missing?\n",
    "print(current_time(),'Count birth year missing before cleaning:',\n",
    "      df_trip.filter((df_trip[\"birth year\"] == \"\") | df_trip[\"birth year\"].isNull() | isnan(df_trip[\"birth year\"])).count())\n",
    "\n",
    "# Cleaning the data\n",
    "df_trip = df_trip.withColumn(\"birth year\", when(df_trip['birth year'] < 1916, None) \\\n",
    "    .otherwise(df_trip['birth year'])\n",
    "    )\n",
    "\n",
    "# Numbers of people with birth data < 1916 should be 0 now\n",
    "print(current_time(),'Count birth year below 1916 after cleaning:',\n",
    "      df_trip.select('birth year').where(df_trip['birth year'] < 1916).count())\n",
    "\n",
    "# Numbers of missing data points should be increased\n",
    "print(current_time(),'Count birth year missing after cleaning:',\n",
    "      df_trip.filter((df_trip[\"birth year\"] == \"\") | df_trip[\"birth year\"].isNull() | isnan(df_trip[\"birth year\"])).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Weather data\n",
    "\n",
    "The weather data only contains daily weather data for one year and one location. So the dataset is quite small and can be assessed in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0  precipitation_hours  precipitation_sum  temperature_2m_max\n",
      "count  366.000000           366.000000         366.000000          366.000000\n",
      "mean   182.500000             2.918033           2.454918           17.057650\n",
      "std    105.799338             4.618468           5.403547           10.109954\n",
      "min      0.000000             0.000000           0.000000           -9.600000\n",
      "25%     91.250000             0.000000           0.000000            9.000000\n",
      "50%    182.500000             0.000000           0.000000           16.800000\n",
      "75%    273.750000             5.000000           2.000000           26.500000\n",
      "max    365.000000            24.000000          44.000000           35.000000\n"
     ]
    }
   ],
   "source": [
    "# read .csv-file of the weather data into pandas df\n",
    "df_weather_nyc = pd.read_csv('stage/weather_data/2016-nyc-weather.csv')  \n",
    "\n",
    "# Inspect the data\n",
    "print(df_weather_nyc.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Inspection of the data shows that **data seems to be in reasonable ranges**. For example minimum and maximums of *precipitation_hours* range from 0 to 24 hours and *temperature_2m_max* seems to be in reasonable celsius ranges.\n",
    "\n",
    "Still data is additionality checked for possible missing values skewing the data as well as completeness of all 366 days of 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:38:24 - Checking weather df for missing-values\n",
      "16:38:24 - Check passed\n"
     ]
    }
   ],
   "source": [
    "# checking for missing values\n",
    "print(current_time(), \"- Checking weather df for missing-values\")\n",
    "if df_weather_nyc.isnull().values.any() == False:\n",
    "    print(current_time(), \"- Check passed\")\n",
    "else:\n",
    "    print(current_time(), \"- Check failed\")\n",
    "    raise Exception(\"df has unexpected missing values. Check api query.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:38:24 - Checking weather df for completeness\n",
      "16:38:24 - Check passed\n"
     ]
    }
   ],
   "source": [
    "# checking if the daily weather data has as many days as the year (2016 = 366 days)\n",
    "print(current_time(), \"- Checking weather df for completeness\")\n",
    "if df_weather_nyc.time.nunique() == 366:\n",
    "    print(current_time(), \"- Check passed\")\n",
    "else:\n",
    "    print(current_time(), \"- Check failed\")\n",
    "    raise Exception(\"Number of unique days in df does not match days of the year. Check for missing days or duplicates genereated in the api query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "\n",
    "After exploring and cleaning the data a data is transformed into a relational data model.\n",
    "\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "To allow users of the database to carry out flexible analysis an relational database model with one fact table (trip_f) and five dimension tables is designed.\n",
    "\n",
    "![Data-Model](data-model.png \"Data Model\")\n",
    "\n",
    "The fact table *trip_f* and the dimension tables *station_d*, *bike_d* and *gender_d* are derived from the trip data. The dimension table *weather_d* is constructed from the weather data. The dimension table *calendar_d* is derived from a combination of trip data and weather data. \n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "Within the data pipelines  the following steps are taken or already have been executed.\n",
    "\n",
    "1. Loading data from S3 and API and saving the raw data in /stage as csv-files. One file for the weather data and monthly files for the trip data.\n",
    "2. Loading multiple csv-files from /stage into aggregated two data frames (trip and weather) with Python/PySpark\n",
    "3. Checking and cleaning the data with Python/PySpark\n",
    "4. Transforming the refined data into data frames representing one fact tables and five dimension tables with PySpark\n",
    "5. Saving each table as a parquet-file in /dwh\n",
    "6. Performing a number of quality checks on the parquet-files to ensure integrity of the data with PySpark\n",
    "\n",
    "![etl](etl.png \"ETL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# A spark session is already active. Create a temporary spark view in order to run spark.sql queries on the trip data\n",
    "\n",
    "df_trip.createOrReplaceTempView(\"trip_v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------+------------+--------------------+\n",
      "|station_id|        station_name|station_lat|station_long|           insert_ts|\n",
      "+----------+--------------------+-----------+------------+--------------------+\n",
      "|      3246|Montague St & Cli...|  40.694283|    -73.9923|2023-01-13 16:38:...|\n",
      "|      3244|University Pl & E...|  40.731438|    -73.9949|2023-01-13 16:38:...|\n",
      "|      3243|     E 58 St & 1 Ave|  40.758923|  -73.962265|2023-01-13 16:38:...|\n",
      "|      3242|Schermerhorn St &...|   40.69103|   -73.99184|2023-01-13 16:38:...|\n",
      "|      3241|Monroe St & Tompk...|  40.686203|  -73.944695|2023-01-13 16:38:...|\n",
      "|      3240|NYCBS Depot BAL -...|        0.0|         0.0|2023-01-13 16:38:...|\n",
      "|      3238|     E 80 St & 2 Ave|  40.773914|    -73.9544|2023-01-13 16:38:...|\n",
      "|      3237|      21 St & 41 Ave|  40.753834|   -73.94268|2023-01-13 16:38:...|\n",
      "|      3236|  W 42 St & Dyer Ave|  40.758984|    -73.9938|2023-01-13 16:38:...|\n",
      "|      3235|E 41 St & Madison...|  40.752167|   -73.97992|2023-01-13 16:38:...|\n",
      "|      3234|E 40 St & Madison...|  40.751595|   -73.98043|2023-01-13 16:38:...|\n",
      "|      3233|     E 48 St & 5 Ave|  40.757244|   -73.97806|2023-01-13 16:38:...|\n",
      "|      3232| Bond St & Fulton St|   40.68962|   -73.98304|2023-01-13 16:38:...|\n",
      "|      3231|  E 67 St & Park Ave|    40.7678|   -73.96592|2023-01-13 16:38:...|\n",
      "|      3230|  Penn Station Valet|   40.75128|  -73.996925|2023-01-13 16:38:...|\n",
      "|      3229|Marcy Ave & MacDo...|   40.68062|   -73.94625|2023-01-13 16:38:...|\n",
      "|      3226|W 82 St & Central...|   40.78275|   -73.97137|2023-01-13 16:38:...|\n",
      "|      3224| W 13 St & Hudson St|  40.739975|   -74.00514|2023-01-13 16:38:...|\n",
      "|      3223|     E 55 St & 3 Ave|  40.758995|   -73.96865|2023-01-13 16:38:...|\n",
      "|      3222|Hanson Pl & St Fe...|  40.685158|   -73.97711|2023-01-13 16:38:...|\n",
      "+----------+--------------------+-----------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      " |-- station_lat: float (nullable = true)\n",
      " |-- station_long: float (nullable = true)\n",
      " |-- insert_ts: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the station dimension table from all unique stations a trip had as start point or end point\n",
    "# for some station_ids mutiple station_names exist, they are kept unique with a window function and sub-selects\n",
    "\n",
    "df_station_d = spark.sql(\"\"\"\n",
    "    SELECT station_id,\n",
    "           station_name,\n",
    "           station_lat,\n",
    "           station_long,\n",
    "           insert_ts\n",
    "    FROM (\n",
    "        SELECT *, row_number( ) over (partition by station_id order by station_name desc) as row_num\n",
    "        FROM (\n",
    "            SELECT DISTINCT CAST(station_id as INTEGER) as station_id,\n",
    "                   station_name,\n",
    "                   ROUND(CAST(station_lat as FLOAT),6) as station_lat,\n",
    "                   ROUND(CAST(station_long as FLOAT),6) as station_long,\n",
    "                   NOW() as insert_ts\n",
    "\n",
    "            FROM (\n",
    "                SELECT `Start Station ID` AS station_id, \n",
    "                       `Start Station Name` AS station_name,\n",
    "                       `Start Station Latitude` AS station_lat,\n",
    "                       `Start Station Longitude` AS station_long\n",
    "                FROM trip_v \n",
    "\n",
    "                UNION ALL\n",
    "\n",
    "                SELECT `End Station ID` AS station_id, \n",
    "                       `End Station Name` AS station_name,\n",
    "                       `End Station Latitude` AS station_lat,\n",
    "                       `End Station Longitude` AS station_long\n",
    "                FROM trip_v \n",
    "            )\n",
    "            WHERE station_id IS NOT NULL    \n",
    "        )\n",
    "    )\n",
    "    WHERE row_num = 1\n",
    "    ORDER BY station_id DESC\n",
    "    \"\"\")\n",
    "\n",
    "# inspect the table\n",
    "df_station_d.show()\n",
    "df_station_d.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+--------------------+\n",
      "|bike_id|  bike_trip_last_ts|bike_trip_cnt|           insert_ts|\n",
      "+-------+-------------------+-------------+--------------------+\n",
      "|  14529|2016-01-31 21:14:22|          294|2023-01-13 16:39:...|\n",
      "|  14530|2016-01-31 20:37:29|          218|2023-01-13 16:39:...|\n",
      "|  14531|2016-01-31 14:07:39|          219|2023-01-13 16:39:...|\n",
      "|  14532|2016-01-31 17:01:34|          208|2023-01-13 16:39:...|\n",
      "|  14533|2016-01-31 22:10:29|          179|2023-01-13 16:39:...|\n",
      "|  14534|2016-01-31 19:20:16|          180|2023-01-13 16:39:...|\n",
      "|  14535|2016-01-31 19:00:43|          280|2023-01-13 16:39:...|\n",
      "|  14536|2016-01-31 21:09:36|          205|2023-01-13 16:39:...|\n",
      "|  14537|2016-01-31 12:18:56|          267|2023-01-13 16:39:...|\n",
      "|  14538|2016-01-31 18:48:18|          212|2023-01-13 16:39:...|\n",
      "|  14539|2016-01-31 20:58:01|          253|2023-01-13 16:39:...|\n",
      "|  14540|2016-01-31 17:11:34|          190|2023-01-13 16:39:...|\n",
      "|  14541|2016-01-31 20:13:56|           55|2023-01-13 16:39:...|\n",
      "|  14544|2016-01-31 22:24:21|          163|2023-01-13 16:39:...|\n",
      "|  14547|2016-01-31 19:12:35|          239|2023-01-13 16:39:...|\n",
      "|  14548|2016-01-31 13:16:03|          121|2023-01-13 16:39:...|\n",
      "|  14549|2016-01-31 22:03:59|          247|2023-01-13 16:39:...|\n",
      "|  14550|2016-01-31 22:45:47|          286|2023-01-13 16:39:...|\n",
      "|  14551|2016-01-31 21:05:03|          243|2023-01-13 16:39:...|\n",
      "|  14552|2016-01-31 21:49:15|          174|2023-01-13 16:39:...|\n",
      "+-------+-------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- bike_id: integer (nullable = true)\n",
      " |-- bike_trip_last_ts: timestamp (nullable = true)\n",
      " |-- bike_trip_cnt: integer (nullable = false)\n",
      " |-- insert_ts: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the bike dimension table from all unique stations booked for a trip \n",
    "df_bike_d = spark.sql(\"\"\"\n",
    "    SELECT  CAST(bikeid as INTEGER) AS bike_id,\n",
    "            MAX(to_timestamp(stoptime, \"mm/dd/yyyy HH:mm:ss\")) AS bike_trip_last_ts,\n",
    "            CAST(COUNT(*) as INTEGER) AS bike_trip_cnt,\n",
    "            NOW() as insert_ts\n",
    "\n",
    "    FROM trip_v \n",
    "    GROUP BY bike_id\n",
    "    ORDER BY bike_id ASC\n",
    "    \"\"\")\n",
    "\n",
    "# inspect the table\n",
    "df_bike_d.show()\n",
    "df_bike_d.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+----------------+--------------------+\n",
      "|gender_id|gender_name_short|gender_name_long|           insert_ts|\n",
      "+---------+-----------------+----------------+--------------------+\n",
      "|        0|             null|            null|2023-01-13 16:39:...|\n",
      "|        1|                m|            male|2023-01-13 16:39:...|\n",
      "|        2|                f|          female|2023-01-13 16:39:...|\n",
      "+---------+-----------------+----------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- gender_id: integer (nullable = true)\n",
      " |-- gender_name_short: string (nullable = true)\n",
      " |-- gender_name_long: string (nullable = true)\n",
      " |-- insert_ts: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the station dimension table from the gender codes in the trip data\n",
    "df_gender_d = spark.sql(\"\"\"\n",
    "    SELECT  DISTINCT CAST(`Gender` AS integer) as gender_id,\n",
    "            CASE  WHEN `Gender` = 0 THEN NULL\n",
    "                    WHEN `Gender` = 1 THEN 'm'\n",
    "                    WHEN `Gender` = 2 THEN 'f'\n",
    "                    END AS gender_name_short, \n",
    "            CASE  WHEN `Gender` = 0 THEN NULL\n",
    "                    WHEN `Gender` = 1 THEN 'male'\n",
    "                    WHEN `Gender` = 2 THEN 'female'\n",
    "                    END AS gender_name_long,\n",
    "            NOW() as insert_ts\n",
    "                             \n",
    "    FROM trip_v \n",
    "    ORDER BY gender_id ASC\n",
    "    \"\"\")\n",
    "\n",
    "# inspect the table\n",
    "df_gender_d.show()\n",
    "df_gender_d.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------+-------------------+----------+------------------------+----------------+--------------+-------+----------+---------------+---------+--------------------+\n",
      "|trip_id|           start_ts|  start_dt|            stop_ts|   stop_dt|trip_duration_in_seconds|start_station_id|end_station_id|bike_id| user_type|user_birth_year|gender_id|           insert_ts|\n",
      "+-------+-------------------+----------+-------------------+----------+------------------------+----------------+--------------+-------+----------+---------------+---------+--------------------+\n",
      "| 509479|2016-01-01 00:00:08|2016-01-01|2016-01-01 00:07:49|2016-01-01|                     461|             480|           524|  23292|Subscriber|           1966|        1|2023-01-13 16:39:...|\n",
      "|1070353|2016-01-01 00:00:23|2016-01-01|2016-01-01 00:02:52|2016-01-01|                     149|             479|           449|  22887|Subscriber|           1991|        1|2023-01-13 16:39:...|\n",
      "|1070354|2016-01-01 00:00:25|2016-01-01|2016-01-01 00:09:37|2016-01-01|                     552|             380|           483|  22939|Subscriber|           1986|        1|2023-01-13 16:39:...|\n",
      "|1070355|2016-01-01 00:00:34|2016-01-01|2016-01-01 00:10:48|2016-01-01|                     613|             447|           281|  20249|Subscriber|           1983|        1|2023-01-13 16:39:...|\n",
      "|      1|2016-01-01 00:00:41|2016-01-01|2016-01-01 00:16:04|2016-01-01|                     923|             268|          3002|  22285|Subscriber|           1958|        1|2023-01-13 16:39:...|\n",
      "|1070356|2016-01-01 00:00:41|2016-01-01|2016-01-01 00:09:18|2016-01-01|                     517|             435|           379|  15969|Subscriber|           1984|        1|2023-01-13 16:39:...|\n",
      "|1070357|2016-01-01 00:00:44|2016-01-01|2016-01-01 00:28:18|2016-01-01|                    1653|             482|           334|  17852|Subscriber|           1977|        1|2023-01-13 16:39:...|\n",
      "|      2|2016-01-01 00:00:45|2016-01-01|2016-01-01 00:07:04|2016-01-01|                     379|             476|           498|  17827|Subscriber|           1969|        1|2023-01-13 16:39:...|\n",
      "|      3|2016-01-01 00:00:48|2016-01-01|2016-01-01 00:10:37|2016-01-01|                     589|             489|           284|  21997|Subscriber|           1982|        2|2023-01-13 16:39:...|\n",
      "| 509480|2016-01-01 00:00:56|2016-01-01|2016-01-01 00:05:53|2016-01-01|                     297|             463|           380|  15329|Subscriber|           1977|        1|2023-01-13 16:39:...|\n",
      "| 509481|2016-01-01 00:01:00|2016-01-01|2016-01-01 00:05:40|2016-01-01|                     280|            3134|          3141|  22927|Subscriber|           1987|        1|2023-01-13 16:39:...|\n",
      "| 509482|2016-01-01 00:01:00|2016-01-01|2016-01-01 00:12:02|2016-01-01|                     662|             537|           428|  20903|Subscriber|           1983|        2|2023-01-13 16:39:...|\n",
      "|      4|2016-01-01 00:01:06|2016-01-01|2016-01-01 00:15:56|2016-01-01|                     889|             268|          3002|  22794|Subscriber|           1961|        2|2023-01-13 16:39:...|\n",
      "|      5|2016-01-01 00:01:12|2016-01-01|2016-01-01 00:25:52|2016-01-01|                    1480|            2006|          2006|  14562|Subscriber|           1952|        1|2023-01-13 16:39:...|\n",
      "|      6|2016-01-01 00:01:19|2016-01-01|2016-01-01 00:06:51|2016-01-01|                     332|            3104|           389|  15788|Subscriber|           1984|        1|2023-01-13 16:39:...|\n",
      "|      7|2016-01-01 00:01:27|2016-01-01|2016-01-01 00:40:08|2016-01-01|                    2321|            3167|          3164|  24183|Subscriber|           1963|        1|2023-01-13 16:39:...|\n",
      "|1070358|2016-01-01 00:01:38|2016-01-01|2016-01-01 00:11:08|2016-01-01|                     570|             297|           511|  17229|Subscriber|           1994|        2|2023-01-13 16:39:...|\n",
      "| 509483|2016-01-01 00:01:41|2016-01-01|2016-01-01 00:07:36|2016-01-01|                     355|             284|           521|  23228|Subscriber|           1978|        1|2023-01-13 16:39:...|\n",
      "|1070359|2016-01-01 00:01:43|2016-01-01|2016-01-01 00:24:12|2016-01-01|                    1348|             479|           128|  20975|  Customer|           null|        0|2023-01-13 16:39:...|\n",
      "| 509484|2016-01-01 00:01:44|2016-01-01|2016-01-01 00:18:58|2016-01-01|                    1034|             223|           305|  23761|Subscriber|           1966|        1|2023-01-13 16:39:...|\n",
      "+-------+-------------------+----------+-------------------+----------+------------------------+----------------+--------------+-------+----------+---------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- trip_id: integer (nullable = true)\n",
      " |-- start_ts: timestamp (nullable = true)\n",
      " |-- start_dt: date (nullable = true)\n",
      " |-- stop_ts: timestamp (nullable = true)\n",
      " |-- stop_dt: date (nullable = true)\n",
      " |-- trip_duration_in_seconds: long (nullable = true)\n",
      " |-- start_station_id: integer (nullable = true)\n",
      " |-- end_station_id: integer (nullable = true)\n",
      " |-- bike_id: integer (nullable = true)\n",
      " |-- user_type: string (nullable = true)\n",
      " |-- user_birth_year: integer (nullable = true)\n",
      " |-- gender_id: integer (nullable = true)\n",
      " |-- insert_ts: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the fact table for trips, remove variables that have been casted to dimension tables\n",
    "df_trip_f = spark.sql(\"\"\"\n",
    "    SELECT  ROW_NUMBER () OVER (ORDER BY starttime) as trip_id,\n",
    "            to_timestamp(starttime, \"mm/dd/yyyy HH:mm:ss\") as start_ts,\n",
    "            DATE(to_timestamp(starttime, \"mm/dd/yyyy HH:mm:ss\")) as start_dt, \n",
    "            to_timestamp(stoptime, \"mm/dd/yyyy HH:mm:ss\") as stop_ts,\n",
    "            DATE(to_timestamp(stoptime, \"mm/dd/yyyy HH:mm:ss\")) as stop_dt,\n",
    "            CAST(tripduration AS LONG) as trip_duration_in_seconds,\n",
    "            CAST(`Start Station ID` AS INTEGER) as start_station_id, \n",
    "            CAST(`End Station ID` AS INTEGER) as end_station_id,\n",
    "            CAST(bikeid AS integer) as bike_id,\n",
    "            usertype as user_type,\n",
    "            CAST(`Birth Year` AS integer) as user_birth_year,\n",
    "            CAST(`Gender` AS integer) as gender_id,\n",
    "            NOW() as insert_ts            \n",
    "            \n",
    "    FROM trip_v \n",
    "    ORDER BY start_ts ASC\n",
    "    \"\"\")\n",
    "\n",
    "# inspect the table\n",
    "df_trip_f.show()\n",
    "df_trip_f.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:40:06 - Started reading staging weather data into spark dataframe\n",
      "16:40:06 - Finished reading 1990273 rows from trip weather data into spark dataframe\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- precipitation_hours: string (nullable = true)\n",
      " |-- precipitation_sum: string (nullable = true)\n",
      " |-- temperature_2m_max: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load weather data into Spark\n",
    "print(current_time(), \"- Started reading staging weather data into spark dataframe\")\n",
    "\n",
    "df_weather = spark.read \\\n",
    "    .option(\"header\",True) \\\n",
    "    .csv('stage/weather_data/')\n",
    "\n",
    "print(current_time(), \"- Finished reading\", df_trip.count(), \"rows from trip weather data into spark dataframe\")\n",
    "\n",
    "# Inspect columns\n",
    "df_weather.printSchema()\n",
    "\n",
    "# Create a temopary spark view in order to run spark.sql queries on the weather data\n",
    "df_weather.createOrReplaceTempView(\"weather_v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-----------------------+--------------------------+--------------------+\n",
      "|    cal_dt|precipitation_hours|precipitation_sum_in_mm|temperature_max_in_celsius|           insert_ts|\n",
      "+----------+-------------------+-----------------------+--------------------------+--------------------+\n",
      "|2016-01-16|                9.0|                    7.7|                      10.0|2023-01-13 16:40:...|\n",
      "|2016-08-25|                0.0|                    0.0|                      29.5|2023-01-13 16:40:...|\n",
      "|2016-07-03|                0.0|                    0.0|                      27.3|2023-01-13 16:40:...|\n",
      "|2016-09-29|                9.0|                    3.7|                      18.2|2023-01-13 16:40:...|\n",
      "|2016-01-14|                1.0|                    0.1|                       2.7|2023-01-13 16:40:...|\n",
      "|2016-02-15|               11.0|                    7.8|                       4.7|2023-01-13 16:40:...|\n",
      "|2016-12-15|                0.0|                    0.0|                      -0.3|2023-01-13 16:40:...|\n",
      "|2016-09-21|                0.0|                    0.0|                      27.5|2023-01-13 16:40:...|\n",
      "|2016-05-24|               15.0|                   10.1|                      22.5|2023-01-13 16:40:...|\n",
      "|2016-08-16|               10.0|                   11.5|                      29.1|2023-01-13 16:40:...|\n",
      "|2016-12-01|                4.0|                    5.5|                      14.7|2023-01-13 16:40:...|\n",
      "|2016-03-26|                0.0|                    0.0|                      10.6|2023-01-13 16:40:...|\n",
      "|2016-05-26|                0.0|                    0.0|                      30.3|2023-01-13 16:40:...|\n",
      "|2016-06-18|                0.0|                    0.0|                      28.8|2023-01-13 16:40:...|\n",
      "|2016-10-28|                0.0|                    0.0|                      13.4|2023-01-13 16:40:...|\n",
      "|2016-07-12|                0.0|                    0.0|                      26.9|2023-01-13 16:40:...|\n",
      "|2016-06-29|                3.0|                    1.4|                      28.0|2023-01-13 16:40:...|\n",
      "|2016-11-16|                0.0|                    0.0|                      15.8|2023-01-13 16:40:...|\n",
      "|2016-08-07|                0.0|                    0.0|                      30.2|2023-01-13 16:40:...|\n",
      "|2016-02-03|               10.0|                   14.1|                      12.3|2023-01-13 16:40:...|\n",
      "+----------+-------------------+-----------------------+--------------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- cal_dt: date (nullable = true)\n",
      " |-- precipitation_hours: float (nullable = true)\n",
      " |-- precipitation_sum_in_mm: float (nullable = true)\n",
      " |-- temperature_max_in_celsius: float (nullable = true)\n",
      " |-- insert_ts: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the weather dimension table\n",
    "df_weather_d = spark.sql(\"\"\"\n",
    "    SELECT  DISTINCT DATE(time) as cal_dt, \n",
    "            CAST(precipitation_hours AS FLOAT),\n",
    "            CAST(precipitation_sum  AS FLOAT) AS precipitation_sum_in_mm,\n",
    "            CAST(temperature_2m_max AS FLOAT) AS temperature_max_in_celsius,\n",
    "            NOW() as insert_ts            \n",
    "            \n",
    "    FROM weather_v \n",
    "    \"\"\")\n",
    "\n",
    "# Inspect the table\n",
    "df_weather_d.show()\n",
    "df_weather_d.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+---------+-------+---------------+--------------------+\n",
      "|    cal_dt|cal_year|cal_quarter|cal_month|cal_day|cal_day_of_week|           insert_ts|\n",
      "+----------+--------+-----------+---------+-------+---------------+--------------------+\n",
      "|2016-01-01|    2016|          1|        1|      1|              6|2023-01-13 16:40:...|\n",
      "|2016-01-02|    2016|          1|        1|      2|              7|2023-01-13 16:40:...|\n",
      "|2016-01-03|    2016|          1|        1|      3|              1|2023-01-13 16:40:...|\n",
      "|2016-01-04|    2016|          1|        1|      4|              2|2023-01-13 16:40:...|\n",
      "|2016-01-05|    2016|          1|        1|      5|              3|2023-01-13 16:40:...|\n",
      "|2016-01-06|    2016|          1|        1|      6|              4|2023-01-13 16:40:...|\n",
      "|2016-01-07|    2016|          1|        1|      7|              5|2023-01-13 16:40:...|\n",
      "|2016-01-08|    2016|          1|        1|      8|              6|2023-01-13 16:40:...|\n",
      "|2016-01-09|    2016|          1|        1|      9|              7|2023-01-13 16:40:...|\n",
      "|2016-01-10|    2016|          1|        1|     10|              1|2023-01-13 16:40:...|\n",
      "|2016-01-11|    2016|          1|        1|     11|              2|2023-01-13 16:40:...|\n",
      "|2016-01-12|    2016|          1|        1|     12|              3|2023-01-13 16:40:...|\n",
      "|2016-01-13|    2016|          1|        1|     13|              4|2023-01-13 16:40:...|\n",
      "|2016-01-14|    2016|          1|        1|     14|              5|2023-01-13 16:40:...|\n",
      "|2016-01-15|    2016|          1|        1|     15|              6|2023-01-13 16:40:...|\n",
      "|2016-01-16|    2016|          1|        1|     16|              7|2023-01-13 16:40:...|\n",
      "|2016-01-17|    2016|          1|        1|     17|              1|2023-01-13 16:40:...|\n",
      "|2016-01-18|    2016|          1|        1|     18|              2|2023-01-13 16:40:...|\n",
      "|2016-01-19|    2016|          1|        1|     19|              3|2023-01-13 16:40:...|\n",
      "|2016-01-20|    2016|          1|        1|     20|              4|2023-01-13 16:40:...|\n",
      "+----------+--------+-----------+---------+-------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- cal_dt: date (nullable = true)\n",
      " |-- cal_year: integer (nullable = true)\n",
      " |-- cal_quarter: integer (nullable = true)\n",
      " |-- cal_month: integer (nullable = true)\n",
      " |-- cal_day: integer (nullable = true)\n",
      " |-- cal_day_of_week: integer (nullable = true)\n",
      " |-- insert_ts: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the calendar dimension table using unique dates from weather data and trip data\n",
    "df_calendar_d = spark.sql(\"\"\"\n",
    "    SELECT  DISTINCT cal_dt,\n",
    "            EXTRACT(year from cal_dt) as cal_year,\n",
    "            EXTRACT(quarter from cal_dt) as cal_quarter,\n",
    "            EXTRACT(month from cal_dt) as cal_month,\n",
    "            EXTRACT(day from cal_dt) as cal_day,\n",
    "            EXTRACT(dayofweek from cal_dt) as cal_day_of_week,\n",
    "            NOW() as insert_ts            \n",
    "            \n",
    "    FROM (\n",
    "        SELECT DISTINCT DATE(time) as cal_dt\n",
    "        FROM weather_v \n",
    "\n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT DISTINCT DATE(to_timestamp(starttime, \"mm/dd/yyyy HH:mm:ss\")) as cal_dt\n",
    "        FROM trip_v\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT DISTINCT DATE(to_timestamp(stoptime, \"mm/dd/yyyy HH:mm:ss\")) as cal_dt\n",
    "        FROM trip_v    \n",
    "\n",
    "    )\n",
    "    \n",
    "    ORDER BY cal_dt ASC\n",
    "    \"\"\")\n",
    "\n",
    "# Inspect the table\n",
    "df_calendar_d.show()\n",
    "df_calendar_d.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:40:36 - Starting to write 6 parquet-files\n",
      "16:40:36 - Reading DataFrame[bike_id: int, bike_trip_last_ts: timestamp, bike_trip_cnt: int, insert_ts: timestamp]\n",
      "16:40:36 - Writing dwh/bike_d.parquet\n",
      "16:40:59 - Reading DataFrame[station_id: int, station_name: string, station_lat: float, station_long: float, insert_ts: timestamp]\n",
      "16:40:59 - Writing dwh/station_d.parquet\n",
      "16:41:39 - Reading DataFrame[cal_dt: date, cal_year: int, cal_quarter: int, cal_month: int, cal_day: int, cal_day_of_week: int, insert_ts: timestamp]\n",
      "16:41:39 - Writing dwh/calendar_d.parquet\n",
      "16:42:09 - Reading DataFrame[trip_id: int, start_ts: timestamp, start_dt: date, stop_ts: timestamp, stop_dt: date, trip_duration_in_seconds: bigint, start_station_id: int, end_station_id: int, bike_id: int, user_type: string, user_birth_year: int, gender_id: int, insert_ts: timestamp]\n",
      "16:42:09 - Writing dwh/trip_f.parquet\n",
      "16:42:52 - Reading DataFrame[gender_id: int, gender_name_short: string, gender_name_long: string, insert_ts: timestamp]\n",
      "16:42:52 - Writing dwh/gender_d.parquet\n",
      "16:43:00 - Reading DataFrame[cal_dt: date, precipitation_hours: float, precipitation_sum_in_mm: float, temperature_max_in_celsius: float, insert_ts: timestamp]\n",
      "16:43:00 - Writing dwh/weather_d.parquet\n",
      "16:43:06 - Finished to write 6 parquet-files\n"
     ]
    }
   ],
   "source": [
    "# Create the dwh-folder if it not already exists\n",
    "os.makedirs('dwh', exist_ok=True)  \n",
    "\n",
    "# List all data frames that should be written into a parqurt-file\n",
    "list_dwh_df=[\n",
    "    {\"df\": df_bike_d, \"table_name\": 'bike_d'},\n",
    "    {\"df\": df_station_d, \"table_name\": 'station_d'},    \n",
    "    {\"df\": df_calendar_d, \"table_name\": 'calendar_d'},\n",
    "    {\"df\": df_trip_f, \"table_name\": 'trip_f'},\n",
    "    {\"df\": df_gender_d, \"table_name\": 'gender_d'},\n",
    "    {\"df\": df_weather_d, \"table_name\": 'weather_d'},\n",
    "]\n",
    "\n",
    "print(current_time(), '- Starting to write', len(list_dwh_df), 'parquet-files')\n",
    "    \n",
    "# loop over list and dict to write to parquet-files\n",
    "for elem in list_dwh_df:\n",
    "    print(current_time(), \"- Reading\", elem['df'])\n",
    "    \n",
    "    # target path\n",
    "    parquet_path = 'dwh/' + elem['table_name'] + '.parquet'\n",
    "    \n",
    "    print(current_time(), \"- Writing\", parquet_path)    \n",
    "    elem['df'].write.mode(\"overwrite\").parquet(parquet_path)\n",
    "\n",
    "print(current_time(), '- Finished to write', len(list_dwh_df), 'parquet-files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "The tables have now been saved into 6 parquet-files. To ensure integrity and data quality each file is read again and checked for existance, completeness and integrity of the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:43:06 - Starting qualtiy check for 6 tables\n",
      "16:43:06 - Checking dwh/bike_d.parquet\n",
      "16:43:06 - 1/4 passed - dwh/bike_d.parquet exists and has succesfully been read\n",
      "16:43:07 - 2/4 passed - Rows in table: 7771 | Rows needed to pass: 10\n",
      "16:43:10 - 3/4 passed - Key bike_id is unique\n",
      "16:43:10 - 4/4 passed - Key dtype in table: int | Key dtype expected: int\n",
      "16:43:10 - Checking dwh/calendar_d.parquet\n",
      "16:43:10 - 1/4 passed - dwh/calendar_d.parquet exists and has succesfully been read\n",
      "16:43:11 - 2/4 passed - Rows in table: 366 | Rows needed to pass: 366\n",
      "16:43:13 - 3/4 passed - Key cal_dt is unique\n",
      "16:43:13 - 4/4 passed - Key dtype in table: date | Key dtype expected: date\n",
      "16:43:13 - Checking dwh/station_d.parquet\n",
      "16:43:13 - 1/4 passed - dwh/station_d.parquet exists and has succesfully been read\n",
      "16:43:13 - 2/4 passed - Rows in table: 490 | Rows needed to pass: 10\n",
      "16:43:15 - 3/4 passed - Key station_id is unique\n",
      "16:43:15 - 4/4 passed - Key dtype in table: int | Key dtype expected: int\n",
      "16:43:15 - Checking dwh/trip_f.parquet\n",
      "16:43:16 - 1/4 passed - dwh/trip_f.parquet exists and has succesfully been read\n",
      "16:43:16 - 2/4 passed - Rows in table: 1990273 | Rows needed to pass: 1000\n",
      "16:43:20 - 3/4 passed - Key trip_id is unique\n",
      "16:43:20 - 4/4 passed - Key dtype in table: int | Key dtype expected: int\n",
      "16:43:20 - Checking dwh/gender_d.parquet\n",
      "16:43:20 - 1/4 passed - dwh/gender_d.parquet exists and has succesfully been read\n",
      "16:43:20 - 2/4 passed - Rows in table: 3 | Rows needed to pass: 3\n",
      "16:43:21 - 3/4 passed - Key gender_id is unique\n",
      "16:43:21 - 4/4 passed - Key dtype in table: int | Key dtype expected: int\n",
      "16:43:21 - Checking dwh/weather_d.parquet\n",
      "16:43:21 - 1/4 passed - dwh/weather_d.parquet exists and has succesfully been read\n",
      "16:43:21 - 2/4 passed - Rows in table: 366 | Rows needed to pass: 366\n",
      "16:43:23 - 3/4 passed - Key cal_dt is unique\n",
      "16:43:23 - 4/4 passed - Key dtype in table: date | Key dtype expected: date\n",
      "16:43:23 - Qualitiy check successful for all 6 elements\n"
     ]
    }
   ],
   "source": [
    "# The dictionary in the list includes and defines values for...\n",
    "# - table name (does the table exist?) \n",
    "# - minimum expected rows (has data been written to the table?)\n",
    "# - key name and key data type (does the key column exists and has the right data type and is it unique?)\n",
    "\n",
    "list_qualitiy_check=[\n",
    "    {\"table_name\": 'bike_d', \"min_rows\": 10, \"key\": 'bike_id', \"key_dtype\": 'int'},\n",
    "    {\"table_name\": 'calendar_d', \"min_rows\": 366, \"key\": 'cal_dt', \"key_dtype\": 'date'},\n",
    "    {\"table_name\": 'station_d', \"min_rows\": 10, \"key\": 'station_id', \"key_dtype\": 'int'},        \n",
    "    {\"table_name\": 'trip_f', \"min_rows\": 1000, \"key\": 'trip_id', \"key_dtype\": 'int'},\n",
    "    {\"table_name\": 'gender_d', \"min_rows\": 3, \"key\": 'gender_id', \"key_dtype\": 'int'},\n",
    "    {\"table_name\": 'weather_d', \"min_rows\": 366, \"key\": 'cal_dt', \"key_dtype\": 'date'}\n",
    "]\n",
    "\n",
    "# start quality check\n",
    "print(current_time(), '- Starting qualtiy check for', len(list_qualitiy_check), 'tables')\n",
    "\n",
    "# loop over defined tables\n",
    "for elem in list_qualitiy_check:\n",
    "    \n",
    "    # read parquet-file\n",
    "    file_name = 'dwh/' + elem['table_name'] + '.parquet'\n",
    "    \n",
    "    # Qualtiy Check 1/4: Does the file exist\n",
    "    print(current_time(), '- Checking', file_name)    \n",
    "    df_qc=spark.read.parquet(file_name)\n",
    "    print(current_time(), '- 1/4 passed -', file_name, 'exists and has succesfully been read')  \n",
    "    \n",
    "    # Qualtiy Check 2/4: File contains defined minimum rows\n",
    "    row_count = 0 # reset count\n",
    "    row_count = df_qc.count()\n",
    "    \n",
    "    if row_count >= elem['min_rows']:\n",
    "        print(current_time(), '- 2/4 passed -', 'Rows in table:', row_count, '| Rows needed to pass:', elem['min_rows'])  \n",
    "        \n",
    "    else:\n",
    "        print(current_time(), '- 2/4 failed')          \n",
    "        raise Exception('Minimum rows for table not fulfilled')\n",
    "        \n",
    "    # Qualtiy Check 3/4: Key is unique\n",
    "    if df_qc.select(elem['key']).distinct().count() == df_qc.select(elem['key']).count():\n",
    "        print(current_time(), '- 3/4 passed -', 'Key', elem[\"key\"], 'is unique')  \n",
    "        \n",
    "    else:\n",
    "        print(current_time(), '- 3/4 failed')          \n",
    "        raise Exception('Key is not unique')          \n",
    "\n",
    "    # Qualtiy Check 4/4: Key has right datatype\n",
    "    df_qc_dtype_key = '' # reset\n",
    "    df_qc_dtype_key = dict(df_qc.dtypes)[elem['key']]\n",
    "    \n",
    "    if elem['key_dtype'] == df_qc_dtype_key:\n",
    "        print(current_time(), '- 4/4 passed -', 'Key dtype in table:', df_qc_dtype_key, '| Key dtype expected:', elem['key_dtype'])  \n",
    "        \n",
    "    else:\n",
    "        print(current_time(), '- 4/4 failed')          \n",
    "        raise Exception('Key dtype does not match',df_qc_dtype_key,elem['key_dtype'])    \n",
    "        \n",
    "print(current_time(), '- Qualitiy check successful for all', len(list_qualitiy_check), 'elements')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "Brief documentation of the data included in the dwh-files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3.1 Fact table trip_f\n",
    "\n",
    "The fact table trip_f contains all valid trips registered by Citi Bike New York including information on start point, end point and the user.\n",
    "\n",
    "|column |dtype|description|key|origin|\n",
    "|:---|:---|:---|:---|:---|\n",
    "|trip_id|integer|Unique identifier of a trip|Primary key|Calculated on trip data|\n",
    "|start_ts|timestamp|Timestamp when a user rented and started a trip with a bike||Original trip data|\n",
    "|start_dt|date|Date when a user rented and started a trip with a bike|Foreign key calendar_d.cal_dt and weather_d.cal_dt|Original trip data|\n",
    "|stop_ts|timestamp|Timestamp when a user ended a trip with a bike||Original trip data|\n",
    "|stop_dt|date|Date when a user ended a trip with a bike |Foreign key calendar_d.cal_dt and weather_d.cal_dt|Original trip data|\n",
    "|trip_duration_in_seconds|long|Duration of the trip in seconds||Original trip data|\n",
    "|start_station_id|integer|Station identifier of the station where a user started a trip|Foreign key station_d.station_id|Original trip data|\n",
    "|end_station_id|integer|Station identifier of the station where a user ended a trip|Foreign key station_d.station_id|Original trip data|\n",
    "|bike_id|integer|Identifier of the bike the user rented for the trip|Foreign key bike_d.bike_id|Original trip data|\n",
    "|user_type|string|User type is defined as customer (24-hour pass or 3-day pass user) or subscriber (Annual Member)||Original trip data|\n",
    "|user_birth_year|integer|Year of birth of the user||Original trip data|\n",
    "|gender_id|integer|Identifier of user gender|Foreign key gender_d.gender_id|Original trip data|\n",
    "|insert_ts|timestamp|Technical timestamp marking the point in time where data was inserted into the table||Calculated in ETL|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3.2 Dimension table bike_d\n",
    "\n",
    "The dimension table bike_d contains meta information on usage of the individual bikes.\n",
    "\n",
    "|column|dtype|description|key|origin|\n",
    "|:---|:---|:---|:---|:---|\n",
    "|bike_id|integer|Unique identifier of the bike|Primary key|Original trip data|\n",
    "|bike_trip_last_ts|timestamp|Timestamp when the last trip was taken with the bike||Calculated on trip data|\n",
    "|bike_trip_cnt|integer|Number of trips that have been taken with the bike||Calculated on trip data|\n",
    "|insert_ts|timestamp|Technical timestamp marking the point in time where data was inserted into the table||Calculated in ETL|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3.3 Dimension table station_d\n",
    "\n",
    "The dimension table station_d contains meta information on all bike sharing stations of Citi Bikes.\n",
    "\n",
    "|column|dtype|description|key|origin|\n",
    "|:---|:---|:---|:---|:---|\n",
    "|station_id|integer|Unique identifier of the station|Primary key|Original trip data|\n",
    "|station_name|string|Name of the station||Original trip data|\n",
    "|station_lat|float|Latitude of the location of the station||Original trip data|\n",
    "|station_long|float|Longitude of the location of the station||Original trip data|\n",
    "|insert_ts|timestamp|Technical timestamp marking the point in time where data was inserted into the table||Calculated in ETL|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3.4 Dimension table calendar_d\n",
    "\n",
    "The dimension table station_d contains meta information on all dates used in this project. It can be joined e.g. on the fact table to get information on which day of the week or which quarter a trip was taken.\n",
    "\n",
    "|column |dtype|description|key|origin|\n",
    "|:---|:---|:---|:---|:---|\n",
    "|cal_dt|date|Unique calendar date|Primary key|From trip data and weather data|\n",
    "|cal_year|integer|Year of the calendar date||Calculated from cal_dt|\n",
    "|cal_quarter|integer|Quarter of the calendar date||Calculated from cal_dt|\n",
    "|cal_month|integer|Month of the calendar date||Calculated from cal_dt|\n",
    "|cal_day|integer|Calendar day of the calendar date||Calculated from cal_dt|\n",
    "|cal_day_of_week|integer|Day of the week of the calendar date||Calculated from cal_dt|\n",
    "|insert_ts|timestamp|Technical timestamp marking the point in time where data was inserted into the table||Calculated in ETL|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3.5 Dimension table gender_d\n",
    "\n",
    "The dimension table gender_d resolves the gender_id into speaking variables.\n",
    "\n",
    "|column|dtype|description|key|origin|\n",
    "|:---|:---|:---|:---|:---|\n",
    "|gender_id|integer|Unique identifier of gender|Primary key|Original trip data|\n",
    "|gender_name_short|string|Abbreviation of the gender (m = male and f = female)||Calculated in ETL|\n",
    "|gender_name_long|string|Full name of the gender||Calculated in ETL|\n",
    "|insert_ts|timestamp|Technical timestamp marking the point in time where data was inserted into the table||Calculated in ETL|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3.6 Dimension table weather_d\n",
    "\n",
    "The dimension table weather_d contains daily weather information for New York City.\n",
    "\n",
    "|column |dtype|description|key|origin|\n",
    "|:---|:---|:---|:---|:---|\n",
    "|cal_dt|date|Calendar date of measurement|Primary key|Original weather data|\n",
    "|precipitation_hours|float|The number of hours with rain||Original weather data|\n",
    "|precipitation_sum_in_mm|float|Sum of daily precipitation in mm (including rain, showers and snowfall)||Original weather data|\n",
    "|temperature_max_in_celsius|float|Maximum daily air temperature at 2 meters above ground in celsius||Original weather data|\n",
    "|insert_ts|timestamp|Technical timestamp marking the point in time where data was inserted into the table||Calculated in ETL|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "The data warehouse would now be useable for a data scientist. For example he or she could evaluate on which day of the week the most trips are taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------------+-------------------------+-------------------------+\n",
      "|cal_day_of_week|avg_trips_per_day_of_week|min_trips_per_day_of_week|max_trips_per_day_of_week|\n",
      "+---------------+-------------------------+-------------------------+-------------------------+\n",
      "|              6|                  74833.0|                    65852|                    84256|\n",
      "|              2|                  68358.0|                    59199|                    79880|\n",
      "|              7|                  65589.0|                    46657|                    85077|\n",
      "|              1|                  61284.0|                    46256|                    78947|\n",
      "|              5|                  61025.0|                    39363|                    76870|\n",
      "|              3|                  59379.0|                    49746|                    74431|\n",
      "|              4|                  56674.0|                    46470|                    64582|\n",
      "+---------------+-------------------------+-------------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in needed tables with Spark and create views\n",
    "calendar_d = spark.read.parquet('dwh/calendar_d.parquet')\n",
    "calendar_d.createOrReplaceTempView(\"calendar_d\")\n",
    "\n",
    "trip_f = spark.read.parquet('dwh/trip_f.parquet')\n",
    "trip_f.createOrReplaceTempView(\"trip_f\")\n",
    "\n",
    "# Aggregate the trip data on daily basis\n",
    "trip_f_daily =  spark.sql(\"\"\"\n",
    "    SELECT  t.start_dt\n",
    "            , COUNT(t.trip_id) as cnt_trips\n",
    "    FROM   trip_f t\n",
    "    GROUP BY 1\n",
    "    \"\"\"\n",
    "    )\n",
    "trip_f_daily.createOrReplaceTempView(\"trip_f_daily\")\n",
    "\n",
    "\n",
    "# First analysis\n",
    "df_analysis_day_of_week = spark.sql(\"\"\"\n",
    "    SELECT  cal.cal_day_of_week\n",
    "            , ROUND(AVG(cnt_trips),0) as avg_trips_per_day_of_week\n",
    "            , MIN(cnt_trips) as min_trips_per_day_of_week\n",
    "            , MAX(cnt_trips) as max_trips_per_day_of_week\n",
    "    FROM   trip_f_daily t\n",
    "    LEFT JOIN calendar_d cal\n",
    "        ON t.start_dt = cal.cal_dt\n",
    "    GROUP BY cal.cal_day_of_week\n",
    "    ORDER BY 2 DESC\n",
    "    \"\"\")\n",
    "\n",
    "# Show result\n",
    "df_analysis_day_of_week.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The results show that the on average most trips are taken on saturdays. In a next step we could analyse how minus degrees impact average rental numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|is_freezing|avg_trips|\n",
      "+-----------+---------+\n",
      "|      false|  65560.0|\n",
      "|       true|  60883.0|\n",
      "+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in needed tables with Spark and create views\n",
    "weather_d = spark.read.parquet('dwh/weather_d.parquet')\n",
    "weather_d.createOrReplaceTempView(\"weather_d\")\n",
    "\n",
    "# Seconds analysis\n",
    "df_analysis_temperature = spark.sql(\"\"\"\n",
    "    SELECT  CASE WHEN w.temperature_max_in_celsius < 0 then True else False end as is_freezing\n",
    "            , ROUND(AVG(cnt_trips),0) as avg_trips\n",
    "    FROM   trip_f_daily t\n",
    "    LEFT JOIN weather_d w\n",
    "        ON t.start_dt = w.cal_dt\n",
    "    GROUP BY 1\n",
    "    ORDER BY 2 DESC\n",
    "    \"\"\")\n",
    "\n",
    "# Show result\n",
    "df_analysis_temperature.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "When the maximum temperature on a given day stays below 0 degrees (-> \"is_freezing == True\") slightly less people are renting bikes on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 5.1 Tool and technology choice\n",
    "\n",
    "**Data storage**\n",
    "- Data has been **read from S3  and an API**\n",
    "- Raw input data is safed as a **csv-file in /stage** in order to have the raw data for backup-cases (e.g. outage of the API)\n",
    "- Refined dwh table data is safed as **parquet-files in /dwh** in order to define data types\n",
    "\n",
    "**ETL Tools**\n",
    "- For data processing, cleaning and processing **PySpark** as the project works with a large dataset. Spark excels in processing big data as it utilizes streaming and parallelization and also can be hosted on the cloud. \n",
    "- For smaller datasets within the ETL **base Python3 and Pandas** is used to inspect and transform data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 5.2 Data updates\n",
    "\n",
    "Currently data is provided monthly by *Citi Bikes* and - theoretically - daily by *Open-Meteo*. Therefore, the data should be updated monthly in order to keep the database up to date. However, in cooperation with *Citi Bikes* maybe a higher frequency of updates ranging from weekly to real-time could be realized. This would enable data scientists and user users to provide faster feedback to *Citi Bikes* and therefore enable *Citi Bikes* to faster react to current situations or trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 5.3 Design changes under different scenarios\n",
    "\n",
    "#### 5.3.1 The data was increased by 100x\n",
    "- Currently the data pipeline is executed in local Spark or even standalone Python. To handle larger data volumes the ETL could be **migrated to cloud services such as AWS EMR** and utilize their far superior computing power.\n",
    "- The data should be stored in a real **database such as AWS Redshift** to utilize its computing power and take advantages of its architecture (like MPP or columnar storage). Therefore, correct distribution keys and sort keys would need to be defined.\n",
    "- In addition, the data pipeline could be reconfigured from the current **full load to a incremental load** (e.g. daily or hourly). This would lead to less data that needs to be processed in every ETL iteration.\n",
    "\n",
    "\n",
    "#### 5.3.2 The pipelines would be run on a daily basis by 7 am every day\n",
    "- First of all: As mentioned above the ETL should be migrated from **full load to incremental load** in order to reduce daily load on the ETL infrastructure\n",
    "- An **orchestration tool like Apache Airflow** should be used to control and steer the daily pipelines. Apache Airflow would allow easier debugging, recovery processes in case a data source goes offline and the opportunity to integrate new data sources in a simple way\n",
    "\n",
    "#### 5.3.3 The database needed to be accessed by 100+ people\n",
    "- Currently our \"database\" is a collection of local files. The files are hard to share and a lot of user working on them could lead to high load on the system. A solution would be to host the relational data base on a **cloud database like AWS Redshift**. Redshift allows a high number of active database connections and users at the same time. In addition it is easily scalable if the number of users should increase further.\n",
    "- Furthermore **schemas and user roles** in redshift would allow us to define which users have access to which data. Which would allow us to provide users access to particular parts of the database further improving load balancing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
